{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a853439c-86de-44b4-8e25-ea2109c37c1e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1b4a52-8296-4812-bbb6-ba7a0adfc6e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q kornia\n",
    "!pip install -q wandb\n",
    "!pip install -q torchmetrics\n",
    "!pip install -q einops\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "from math import exp\n",
    "from tqdm import tqdm\n",
    "\n",
    "from kornia.filters.sobel import Sobel\n",
    "import wandb\n",
    "from torchvision.utils import make_grid\n",
    "import gc\n",
    "from torchmetrics import StructuralSimilarityIndexMeasure as SSIM\n",
    "\n",
    "\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from einops import rearrange\n",
    "from torch.nn import init "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a9dadc-a151-442b-a062-97e85e9b1b44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa933957-de3b-4d20-8c4f-4734c9e38d68",
   "metadata": {},
   "source": [
    "# Loss, Metric and Other Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20212ed8-8df3-4967-8e0a-e349c4f01098",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize(sample):\n",
    "    \"\"\"\n",
    "    Normalizes Digital Elevation Model (DEM) data to a range of [0,1] using min-max normalization.\n",
    "    \n",
    "    This function performs feature scaling on DEM elevation values by applying min-max normalization.\n",
    "    The normalization is crucial for DEM super resolution tasks as it helps in:\n",
    "        - Standardizing the elevation values to a common scale\n",
    "        - Improving numerical stability during processing\n",
    "        - Making the data suitable for machine learning models\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sample : numpy.ndarray\n",
    "        Input DEM data array containing elevation values\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Normalized DEM data with values scaled between 0 and 1\n",
    "    \"\"\"\n",
    "    MIN_H = sample.min()\n",
    "    MAX_H = sample.max()\n",
    "    return (sample - MIN_H)/(MAX_H-MIN_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faca0322-ff33-42d3-8f76-99d1519fc929",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_psnr(img1, img2, border=0 ,data_min=0.0 ,data_max=1.0 ):\n",
    "    \"\"\"\n",
    "    Calculates Peak Signal-to-Noise Ratio (PSNR) between two images.\n",
    "    \n",
    "    PSNR is a quality metric that measures the ratio between the maximum possible \n",
    "    signal power and the power of corrupting noise that affects the quality of the\n",
    "    representation. Higher PSNR values indicate better quality.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    img1 : torch.Tensor or numpy.ndarray\n",
    "        First input image (reference/ground truth)\n",
    "    img2 : torch.Tensor or numpy.ndarray\n",
    "        Second input image (predicted/generated)\n",
    "    border : int, optional\n",
    "        Number of border pixels to exclude from calculation (default: 0)\n",
    "    data_min : float, optional\n",
    "        Minimum value of the data range (default: 0.0)\n",
    "    data_max : float, optional\n",
    "        Maximum value of the data range (default: 1.0)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        PSNR value in decibels (dB). Returns infinity if images are identical\n",
    "        \n",
    "    Raises:\n",
    "    -------\n",
    "    ValueError\n",
    "        If input images don't have the same dimensions\n",
    "    \"\"\"\n",
    "    \n",
    "    if not img1.shape == img2.shape:\n",
    "        raise ValueError('Input images must have the same dimensions.')\n",
    "    h, w = img1.shape[2:]\n",
    "\n",
    "    img1 = img1[border:h-border, border:w-border]\n",
    "    img2 = img2[border:h-border, border:w-border]\n",
    "\n",
    "    mse = np.mean((img1 - img2)**2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 20 * math.log10((data_max - data_min)/ math.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3b2046-b9a4-43ca-beef-fe6b6d2838f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class gradientAwareLoss(nn.Module): \n",
    "    \"\"\"\n",
    "    A custom loss function that computes the L1 difference between edge maps of \n",
    "    high-resolution and super-resolved images using Sobel edge detection.\n",
    "    \n",
    "    This loss helps in preserving edge and gradient information during super-resolution,\n",
    "    ensuring better structural fidelity in the output.\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    sobelFilter : Sobel\n",
    "        Sobel edge detection filter implemented on CUDA\n",
    "    l1Loss : nn.L1Loss\n",
    "        L1 loss function implemented on CUDA\n",
    "        \n",
    "    Methods:\n",
    "    --------\n",
    "    forward(hr, sr):\n",
    "        Computes the gradient-aware loss between high-resolution(Ground Truth) and super-resolved(Model's Output) images\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sobelFilter = Sobel().to('cuda')\n",
    "        self.l1Loss = nn.L1Loss().to('cuda')\n",
    "\n",
    "    def forward(self, hr, sr):\n",
    "        hrEdgeMap = self.sobelFilter(hr)\n",
    "        srEdgeMap = self.sobelFilter(sr)\n",
    "        return self.l1Loss(hrEdgeMap, srEdgeMap)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803a8dfa-92e6-43c8-b197-7b8b12a5b38c",
   "metadata": {},
   "source": [
    "# Dataset and DataloaderÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff5589-b2c7-4720-8325-c1f4fc07e9bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Dataset class\n",
    "# class Dataset(data.Dataset):\n",
    "#     def __init__(self, load_dir, normalize = True,transform=transforms.Compose([transforms.ToTensor()])):\n",
    "#         self.load_dir = load_dir\n",
    "#         self.tranform = transform\n",
    "#         self.downsampler = torch.nn.Upsample(scale_factor=0.5, mode='bilinear')\n",
    "#         self.normalize = normalize      \n",
    "#     def __getitem__(self, idx):        \n",
    "#         try:\n",
    "#             if self.normalize:\n",
    "#                 im = normalize(torch.load(self.load_dir[idx]))\n",
    "#             else:\n",
    "#                 im = torch.load(self.dems[idx])\n",
    "#             HR = im.copy().astype(np.float32) \n",
    "#             HR = torch.from_numpy(HR).unsqueeze(0) \n",
    "            \n",
    "#             # print(\"shape of LR\", LR.shape)\n",
    "#             LR = self.downsampler(HR.unsqueeze(0)).squeeze()     \n",
    "#             # print(\"shape of LR\", LR.shape)\n",
    "#             return HR, LR.unsqueeze(0)\n",
    "#         except:\n",
    "#             print(idx)\n",
    "#             print(self.load_dir[idx])\n",
    "#     def __len__(self):\n",
    "#         return len(self.load_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b17e1b-12bc-4c38-88fc-3af2c3171884",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def custom_collate(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to filter out None values\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    batch : list\n",
    "        List of data samples, potentially containing None values\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    torch.Tensor or None\n",
    "        Collated batch after filtering None values, returns None if batch is empty\n",
    "    \"\"\"\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, load_dir, normalize=True, transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                 downsample_methods=['bilinear', 'area', 'gaussian','motion_blur','median_blur']):\n",
    "        \"\"\"\n",
    "        Custom Dataset class for DEM super-resolution training that implements multiple \n",
    "        downsampling methods and data preprocessing.\n",
    "\n",
    "        The dataset handles loading high-resolution DEM images and generates corresponding \n",
    "        low-resolution pairs using various downsampling techniques.\n",
    "        \n",
    "        Parameters:\n",
    "            load_dir (list): List of paths to high-resolution images\n",
    "            normalize (bool): Whether to normalize the images\n",
    "            transform (torchvision.transforms): Additional transformations\n",
    "            downsample_methods (list): List of downsampling methods to use\n",
    "        \"\"\"\n",
    "        self.load_dir = load_dir\n",
    "        self.transform = transform\n",
    "        self.normalize = normalize\n",
    "        self.downsample_methods = downsample_methods\n",
    "        \n",
    "        # Predefined downsampling methods\n",
    "        self.downsamplers = {\n",
    "            'bilinear': self._bilinear_downsample,\n",
    "            'area': self._area_downsample,\n",
    "            'gaussian': self._gaussian_downsample,\n",
    "            'motion_blur': self._motion_blur_downsample,\n",
    "            'median_blur': self._median_blur_downsample\n",
    "        }\n",
    "    \n",
    "    def _bilinear_downsample(self, HR):\n",
    "        \"\"\"Bilinear downsampling\"\"\"\n",
    "        return torch.nn.functional.interpolate(HR.unsqueeze(0), scale_factor=0.5, mode='bilinear').squeeze()\n",
    "    \n",
    "    def _area_downsample(self, HR):\n",
    "        \"\"\"Area interpolation downsampling\"\"\"\n",
    "        return torch.nn.functional.interpolate(HR.unsqueeze(0), scale_factor=0.5, mode='area').squeeze()\n",
    "    \n",
    "    def _gaussian_downsample(self, HR):\n",
    "        \"\"\"Gaussian blur + downsampling\"\"\"\n",
    "        hr_np = HR.squeeze(0).numpy() \n",
    "        blurred = cv2.GaussianBlur(hr_np, (5, 5), 0)\n",
    "        downsampled = cv2.resize(blurred, (HR.shape[2]//2, HR.shape[1]//2), interpolation=cv2.INTER_AREA)\n",
    "        return torch.from_numpy(downsampled).unsqueeze(0) \n",
    "    \n",
    "    def _motion_blur_downsample(self, HR):\n",
    "        \"\"\"Motion blur + downsampling\"\"\"\n",
    "        hr_np = HR.squeeze(0).numpy() \n",
    "        kernel_size = 5\n",
    "        kernel_v = np.zeros((kernel_size, kernel_size))\n",
    "        kernel_v[:, kernel_size // 2] = np.ones(kernel_size)\n",
    "        kernel_v /= kernel_size\n",
    "        blurred = cv2.filter2D(hr_np, -1, kernel_v)\n",
    "        downsampled = cv2.resize(blurred, (HR.shape[2]//2, HR.shape[1]//2), interpolation=cv2.INTER_AREA)\n",
    "        return torch.from_numpy(downsampled).unsqueeze(0)  # Add channel back\n",
    "\n",
    "    \n",
    "    def _median_blur_downsample(self, HR):\n",
    "        \"\"\"Median blur + downsampling\"\"\"\n",
    "        hr_np = HR.squeeze(0).numpy() \n",
    "        blurred = cv2.medianBlur(hr_np.astype(np.uint8), 5)\n",
    "        downsampled = cv2.resize(blurred, (HR.shape[2]//2, HR.shape[1]//2), interpolation=cv2.INTER_AREA)\n",
    "        return torch.from_numpy(downsampled).unsqueeze(0)  # Add channel back\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= len(self):\n",
    "            raise IndexError(f\"Index {idx} is out of range\")\n",
    "        \n",
    "        try:\n",
    "            image_path = self.load_dir[idx]\n",
    "            \n",
    "            if not os.path.exists(image_path):\n",
    "                raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
    "            \n",
    "            im = torch.load(image_path)\n",
    "            \n",
    "            if self.normalize:\n",
    "                im = normalize(im)\n",
    "            \n",
    "            if not isinstance(im, torch.Tensor):\n",
    "                im = torch.tensor(im, dtype=torch.float32)\n",
    "            \n",
    "            if im.dim() == 2:\n",
    "                im = im.unsqueeze(0)\n",
    "            \n",
    "            HR = im.float()\n",
    "            \n",
    "            downsample_method = random.choice(self.downsample_methods)\n",
    "            \n",
    "            LR = self.downsamplers[downsample_method](HR)\n",
    "            \n",
    "            HR = HR.squeeze()\n",
    "            LR = LR.squeeze().float()\n",
    "            \n",
    "            return HR.unsqueeze(0), LR.unsqueeze(0)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image at index {idx}\")\n",
    "            print(f\"Image path: {self.load_dir[idx]}\")\n",
    "            print(f\"Error details: {type(e).__name__}: {str(e)}\")\n",
    "            \n",
    "            return None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.load_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54365802-3a9c-466e-a455-8707c2989c81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Data Directory Setup\n",
    "\"\"\"\n",
    "    Sets up training and testing data directories for DEM super-resolution by loading\n",
    "    PyTorch tensor files (.pt) and performing basic data validation.\n",
    "\n",
    "    The script:\n",
    "    1. Loads training samples from specified training directory\n",
    "    2. Loads testing samples from specified testing directory\n",
    "    3. Removes specific problematic samples if needed\n",
    "    4. Verifies data loading by printing dataset sizes\n",
    "\n",
    "    Variables:\n",
    "    ----------\n",
    "    train_dir : list\n",
    "        List of paths to training DEM tensor files\n",
    "    test_dir : list\n",
    "        List of paths to testing DEM tensor files\n",
    "\"\"\"\n",
    "\n",
    "train_dir = glob.glob('../data_DEMs_no_nan/train/*.pt')\n",
    "test_dir = glob.glob('../data_DEMs_no_nan/test/*.pt')\n",
    "\n",
    "# this line just removed one of the currupted DEM file.\n",
    "# test_dir.remove('../data_DEMs_no_nan/test/NAC_DTM_MESSIER3_block_211.pt')\n",
    "\n",
    "\n",
    "# prints the numnber of samples in trainig and testing dataset.\n",
    "print(len(train_dir)) \n",
    "print(len(test_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94a84ea-b96a-4695-bd8e-fb4358658e46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset and DataLoader Initialization\n",
    "\"\"\"\n",
    "    Initializes training and testing datasets and their respective dataloaders for DEM super-resolution.\n",
    "\n",
    "    The setup includes:\n",
    "    1. Creation of custom Dataset instances for both training and testing\n",
    "    2. Configuration of DataLoaders with specific batch sizes and shuffling parameters\n",
    "    3. Organization of data for efficient model training and evaluation\n",
    "\n",
    "    Variables:\n",
    "    ----------\n",
    "    trainset : Dataset\n",
    "        Custom dataset instance for training data\n",
    "    testset : Dataset\n",
    "        Custom dataset instance for testing data\n",
    "    trainloader : DataLoader\n",
    "        DataLoader for training with batch processing and shuffling\n",
    "    testloader : DataLoader\n",
    "        DataLoader for testing with batch processing\n",
    "\"\"\"\n",
    "\n",
    "trainset = Dataset(load_dir = train_dir)\n",
    "testset  = Dataset(load_dir = test_dir)\n",
    "\n",
    "trainloader = DataLoader(trainset,batch_size=4,shuffle=True)\n",
    "testloader = DataLoader(testset,batch_size=4,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb52159-8aec-4d3a-a9c2-44a9c4d13119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "def visualize_dataloader(dataloader, n_batches=5, mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)):\n",
    "    \"\"\"\n",
    "    Visualize images from a DataLoader.\n",
    "\n",
    "    Args:\n",
    "        dataloader: PyTorch DataLoader object.\n",
    "        n_batches: Number of batches to visualize.\n",
    "        mean: Mean used for normalization.\n",
    "        std: Standard deviation used for normalization.\n",
    "    \"\"\"\n",
    "    batch_count = 0\n",
    "\n",
    "    for _, images in dataloader:\n",
    "        if batch_count >= n_batches:\n",
    "            break\n",
    "        \n",
    "        # Denormalize images if necessary\n",
    "        images = torchvision.utils.make_grid(images, nrow=images.shape[0])\n",
    "        images = images.permute(1, 2, 0).numpy()  # Convert to HWC format\n",
    "        # images = (images * std + mean).clip(0, 1)  # Denormalize and clip\n",
    "        \n",
    "        # Plot the images\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(images)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Batch {batch_count + 1}\")\n",
    "        plt.show()\n",
    "\n",
    "        batch_count += 1\n",
    "visualize_dataloader(trainloader, n_batches=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd36f37-21ae-442e-ab39-65d92c57286d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualization \n",
    "for b, (hr, lr) in enumerate(trainloader):\n",
    "    print(hr.shape)\n",
    "    print(lr.shape)\n",
    "    \n",
    "    print(hr.min())\n",
    "    print(hr.max())\n",
    "    \n",
    "    print(lr.min())\n",
    "    print(lr.max())\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 12))\n",
    "\n",
    "    axes[0].set_yticklabels([])\n",
    "\n",
    "    axes[0].imshow(hr[2].squeeze(), cmap='gray')\n",
    "    axes[1].imshow(lr[2].squeeze(), cmap='gray')\n",
    "    \n",
    "    axes[0].set_title(\"HR\")\n",
    "    axes[1].set_title(\"LR\")\n",
    "    \n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46f1aff-fd4b-454a-85a8-7aa50540f003",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ecf50d-204f-4dce-919b-44dc56cd8fa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class depthwise_separable_conv(nn.Module):\n",
    "    \"\"\"\n",
    "        Implements depthwise separable convolution, which factorizes standard convolution \n",
    "        into depthwise and pointwise operations for efficient computation.\n",
    "\n",
    "        The operation consists of two steps:\n",
    "        1. Depthwise convolution: applies a single filter per input channel\n",
    "        2. Pointwise convolution: applies 1x1 convolution to combine the outputs\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        nin : int\n",
    "            Number of input channels\n",
    "        nout : int\n",
    "            Number of output channels\n",
    "        kernel_size : int, optional\n",
    "            Size of the convolving kernel (default: 3)\n",
    "        padding : int, optional\n",
    "            Padding size (default: 1)\n",
    "        bias : bool, optional\n",
    "            If True, adds a learnable bias to the output (default: False)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nin, nout, kernel_size = 3, padding = 1, bias=False):\n",
    "        super(depthwise_separable_conv, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(nin, nin, kernel_size=kernel_size, padding=padding, groups=nin, bias=bias)\n",
    "        self.pointwise = nn.Conv2d(nin, nout, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "class ERAM(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Enhanced Residual Attention Module (ERAM) for feature refinement in super-resolution.\n",
    "    \n",
    "    Implementation based on:\n",
    "    Li, Y., Zhou, L., Xu, F., & Chen, S. (2022). OGSRN: Optical-guided super-resolution \n",
    "    network for SAR image. Chinese Journal of Aeronautics, 35(5), 204-219.\n",
    "    https://doi.org/10.1016/j.cja.2021.08.036\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    channel_begin : int\n",
    "        Number of input channels\n",
    "    dimension : int\n",
    "        Spatial dimension for average pooling\n",
    "        \n",
    "    Architecture components:\n",
    "    - Channel attention branch with statistical and convolution operations\n",
    "    - Spatial attention branch with depthwise separable convolutions\n",
    "    - Residual connection for feature preservation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, channel_begin, dimension):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channel_begin, channel_begin, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.avgpool = nn.AvgPool2d(dimension)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(channel_begin, channel_begin//2, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(channel_begin//2, channel_begin, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(channel_begin, channel_begin, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.dconv = depthwise_separable_conv(channel_begin, channel_begin, kernel_size = 3, padding = 1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Forward pass combining channel and spatial attention mechanisms.\n",
    "\n",
    "            Parameters:\n",
    "            -----------\n",
    "            x : torch.Tensor\n",
    "                Input feature map\n",
    "\n",
    "            Returns:\n",
    "            --------\n",
    "            torch.Tensor\n",
    "                Refined feature map with attention applied\n",
    "        \"\"\"\n",
    "        # Channel attention\n",
    "        si_ca = self.avgpool(x) + torch.var_mean(x, dim=(2,3))[0].unsqueeze(2).unsqueeze(2)\n",
    "        mi_ca = self.conv2(self.relu(self.conv1(si_ca)))\n",
    "        \n",
    "        # Spatial attention\n",
    "        mi_sa = self.conv3(self.relu(self.dconv(x)))\n",
    "        \n",
    "        # Combine attentions and apply to input\n",
    "        return self.sigmoid(mi_ca+mi_sa) * x\n",
    "\n",
    "    \n",
    "\n",
    "class SelfAttn(nn.Module):\n",
    "    \"\"\"\n",
    "        Self-Attention module implementing multi-head attention mechanism for feature refinement.\n",
    "\n",
    "        This module computes self-attention using queries, keys, and values with multiple attention\n",
    "        heads, allowing the model to attend to different feature aspects in parallel.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        dim : int\n",
    "            Input feature dimension\n",
    "        num_heads : int, optional\n",
    "            Number of attention heads (default: 8)\n",
    "        bias : bool, optional\n",
    "            Whether to include bias in linear transformations (default: False)\n",
    "        \n",
    "        Attributes:\n",
    "        -----------\n",
    "        scale : float\n",
    "            Scaling factor for attention scores\n",
    "        qkv : nn.Linear\n",
    "            Linear projection for computing query, key, and value\n",
    "        proj_out : nn.Linear\n",
    "            Output projection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim, num_heads=8, bias=False):\n",
    "        super(SelfAttn, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=bias)\n",
    "        self.proj_out = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Compute self-attention on input features.\n",
    "\n",
    "            Parameters:\n",
    "            -----------\n",
    "            x : torch.Tensor\n",
    "                Input tensor of shape [batch_size, num_tokens, channels]\n",
    "\n",
    "            Returns:\n",
    "            --------\n",
    "            torch.Tensor\n",
    "                Attention-refined features of shape [batch_size, num_tokens, channels]\n",
    "        \"\"\"\n",
    "        b, N, c = x.shape\n",
    "\n",
    "        qkv = self.qkv(x).chunk(3, dim=-1)\n",
    "        # [b, N, c] -> [b, N, head, c//head] -> [b, head, N, c//head]\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.num_heads), qkv)\n",
    "\n",
    "        # [b, head, N, c//head] * [b, head, N, c//head] -> [b, head, N, N]\n",
    "        attn = torch.einsum('bijc, bikc -> bijk', q, k) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        # [b, head, N, N] * [b, head, N, c//head] -> [b, head, N, c//head] -> [b, N, head, c//head]\n",
    "        x = torch.einsum('bijk, bikc -> bijc', attn, v)\n",
    "        x = rearrange(x, 'b i j c -> b j (i c)')\n",
    "        x = self.proj_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\"\n",
    "        Multi-Layer Perceptron (MLP) module with GELU activation.\n",
    "\n",
    "        This module implements a two-layer feed-forward network with expansion ratio,\n",
    "        commonly used in transformer architectures as the feed-forward network (FFN) component.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        in_features : int\n",
    "            Number of input features\n",
    "        mlp_ratio : int, optional\n",
    "            Expansion ratio for hidden dimension (default: 4)\n",
    "\n",
    "        Attributes:\n",
    "        -----------\n",
    "        fc : nn.Sequential\n",
    "            Sequential container of linear layers and GELU activation\n",
    "            - First linear layer expands dimensions\n",
    "            - GELU activation function\n",
    "            - Second linear layer projects back to input dimension\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, mlp_ratio=4):\n",
    "        super(Mlp, self).__init__()\n",
    "        hidden_features = in_features * mlp_ratio\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_features),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_features, in_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (b, h, w, c)\n",
    "        window_size (int): window size\n",
    "    Returns:\n",
    "        windows: (num_windows*b, window_size, window_size, c) [non-overlap]\n",
    "    \"\"\"\n",
    "    return rearrange(x, 'b (h s1) (w s2) c -> (b h w) s1 s2 c', s1=window_size, s2=window_size)\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, h, w):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        h (int): Height of image\n",
    "        w (int): Width of image\n",
    "    Returns:\n",
    "        x: (b, h, w, c)\n",
    "    \"\"\"\n",
    "    b = int(windows.shape[0] / (h * w / window_size / window_size))\n",
    "    return rearrange(windows, '(b h w) s1 s2 c -> b (h s1) (w s2) c', b=b, h=h // window_size, w=w // window_size)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "        Window-based Transformer module for processing DEM features.\n",
    "\n",
    "        This module combines positional embedding, self-attention within windows,\n",
    "        and MLP layers with skip connections for feature transformation.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        dim : int\n",
    "            Feature dimension\n",
    "        num_heads : int, optional\n",
    "            Number of attention heads (default: 4)\n",
    "        window_size : int, optional\n",
    "            Size of attention windows (default: 8)\n",
    "        mlp_ratio : int, optional\n",
    "            MLP expansion ratio (default: 4)\n",
    "        qkv_bias : bool, optional\n",
    "            Whether to use bias in QKV projection (default: False)\n",
    "\n",
    "        Attributes:\n",
    "        -----------\n",
    "        pos_embed : nn.Conv2d\n",
    "            Depthwise convolution for positional embedding\n",
    "        norm1, norm2 : nn.LayerNorm\n",
    "            Layer normalization modules\n",
    "        attn : SelfAttn\n",
    "            Self-attention module\n",
    "        mlp : Mlp\n",
    "            Multi-layer perceptron module\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=4, window_size=8, mlp_ratio=4, qkv_bias=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.pos_embed = nn.Conv2d(dim, dim, 3, padding=1, groups=dim)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = SelfAttn(dim, num_heads, qkv_bias)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "        self.mlp = Mlp(dim, mlp_ratio)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Forward pass of the Transformer.\n",
    "\n",
    "            Process steps:\n",
    "            1. Add positional embedding\n",
    "            2. Partition input into windows\n",
    "            3. Apply self-attention within windows\n",
    "            4. Merge windows and apply MLP\n",
    "            5. Add skip connections\n",
    "\n",
    "            Parameters:\n",
    "            -----------\n",
    "            x : torch.Tensor\n",
    "                Input features [B, C, H, W]\n",
    "\n",
    "            Returns:\n",
    "            --------\n",
    "            torch.Tensor\n",
    "                Transformed features [B, C, H, W]\n",
    "        \"\"\"\n",
    "        x = x + self.pos_embed(x)\n",
    "        x = rearrange(x, 'b c h w -> b h w c')\n",
    "        b, h, w, c = x.shape\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        pad_l = pad_t = 0\n",
    "        pad_r = (self.window_size - w % self.window_size) % self.window_size\n",
    "        pad_b = (self.window_size - h % self.window_size) % self.window_size\n",
    "        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "        _, Hp, Wp, _ = x.shape\n",
    "\n",
    "        x_windows = window_partition(x, self.window_size)  # nW*B, window_size, window_size, c\n",
    "        x_windows = rearrange(x_windows, 'B s1 s2 c -> B (s1 s2) c', s1=self.window_size,\n",
    "                              s2=self.window_size)  # nW*b, window_size*window_size, c\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows)  # nW*b, window_size*window_size, c\n",
    "        attn_windows = attn_windows.reshape(b*256,64,-1)\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = rearrange(attn_windows, 'B (s1 s2) c -> B s1 s2 c', s1=self.window_size, s2=self.window_size)\n",
    "        x = window_reverse(attn_windows, self.window_size, Hp, Wp)  # b H' W' c\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if pad_r > 0 or pad_b > 0:\n",
    "            x = x[:, :h, :w, :].contiguous()\n",
    "\n",
    "        x = x + shortcut\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return rearrange(x, 'b h w c -> b c h w')\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        Residual Block with grouped convolutions and channel expansion.\n",
    "\n",
    "        This block implements a modified residual structure using:\n",
    "        1. Channel expansion through 1x1 convolution\n",
    "        2. Grouped 3x3 convolution for spatial processing\n",
    "        3. Channel reduction through 1x1 convolution\n",
    "        4. Residual connection\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        in_features : int\n",
    "            Number of input channels\n",
    "        ratio : int, optional\n",
    "            Channel expansion ratio (default: 4)\n",
    "\n",
    "        Attributes:\n",
    "        -----------\n",
    "        net : nn.Sequential\n",
    "            Sequential container of layers:\n",
    "            - First 1x1 conv for channel expansion\n",
    "            - LeakyReLU activation\n",
    "            - Grouped 3x3 conv (groups = in_features * ratio)\n",
    "            - LeakyReLU activation\n",
    "            - Second 1x1 conv for channel reduction\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, ratio=4):\n",
    "        super(ResBlock, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_features, in_features * ratio, 1, 1, 0),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_features * ratio, in_features * ratio, 3, 1, 1, groups=in_features * ratio),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_features * ratio, in_features, 1, 1, 0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x) + x\n",
    "\n",
    "\n",
    "\n",
    "class BaseBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        Base building block combining Transformer, ResBlock, and ERAM modules in sequence.\n",
    "\n",
    "        This block creates a series of processing stages, each consisting of:\n",
    "        1. Window-based Transformer for global feature interaction\n",
    "        2. Residual block for local feature processing\n",
    "        3. Enhanced Residual Attention Module (ERAM) for feature refinement\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        dim : int\n",
    "            Feature dimension\n",
    "        num_heads : int, optional\n",
    "            Number of attention heads in Transformer (default: 8)\n",
    "        window_size : int, optional\n",
    "            Size of attention windows (default: 8)\n",
    "        ratios : list, optional\n",
    "            List of expansion ratios for each stage (default: [1, 2, 2, 4, 4])\n",
    "        qkv_bias : bool, optional\n",
    "            Whether to use bias in Transformer QKV projection (default: False)\n",
    "\n",
    "        Attributes:\n",
    "        -----------\n",
    "        layers : nn.ModuleList\n",
    "            List of processing stages, each containing Transformer, ResBlock, and ERAM\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads=8, window_size=8, ratios=[1, 2, 2, 4, 4], qkv_bias=False):\n",
    "        super(BaseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for ratio in ratios:\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Transformer(dim, num_heads, window_size, ratio, qkv_bias),\n",
    "                ResBlock(dim, ratio),\n",
    "                ERAM(dim,128) # this 128 represent the input square (128x128)dimension, can be changed. \n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for tblock, rblock ,eram in self.layers:\n",
    "            x = tblock(x)\n",
    "            x = rblock(x)\n",
    "            x = eram(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SRModel(nn.Module):\n",
    "    \"\"\"\n",
    "        Super-Resolution Model combining transformer-based processing with pixel shuffle upsampling.\n",
    "\n",
    "        Architecture components:\n",
    "        1. Feature extraction head\n",
    "        2. Transformer-based main body\n",
    "        3. Feature fusion\n",
    "        4. Pixel shuffle upsampling\n",
    "        5. Reconstruction tail\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_feats : int, optional\n",
    "            Number of feature channels (default: 40)\n",
    "        n_heads : int, optional\n",
    "            Number of attention heads (default: 8)\n",
    "        ratios : list, optional\n",
    "            Expansion ratios for BaseBlock stages (default: [4, 2, 2, 2, 4])\n",
    "        upscaling_factor : int, optional\n",
    "            Super-resolution scale factor (default: 2)\n",
    "\n",
    "        Attributes:\n",
    "        -----------\n",
    "        head : nn.Conv2d\n",
    "            Initial feature extraction\n",
    "        body : BaseBlock\n",
    "            Main feature processing block\n",
    "        fuse : nn.Conv2d\n",
    "            Feature fusion layer\n",
    "        upsapling : nn.Sequential\n",
    "            Pixel shuffle-based upsampling network\n",
    "        tail : nn.Conv2d\n",
    "            Final reconstruction layer\n",
    "        act : nn.LeakyReLU\n",
    "            Activation function\n",
    "    \"\"\"\n",
    "    def __init__(self, n_feats=40, n_heads=8, ratios=[4, 2, 2, 2, 4], upscaling_factor=2):\n",
    "        super(SRModel, self).__init__()\n",
    "        self.scale = upscaling_factor\n",
    "        self.head = nn.Conv2d(1, n_feats, 3, 1, 1)\n",
    "\n",
    "        self.body = BaseBlock(n_feats, num_heads=n_heads, ratios=ratios)\n",
    "\n",
    "        self.fuse = nn.Conv2d(n_feats * 2, n_feats, 3, 1, 1)\n",
    "\n",
    "        if self.scale == 4:\n",
    "            self.upsapling = nn.Sequential(\n",
    "                nn.Conv2d(n_feats, n_feats * 4, 1, 1, 0),\n",
    "                nn.PixelShuffle(2),\n",
    "                nn.Conv2d(n_feats, n_feats * 4, 1, 1, 0),\n",
    "                nn.PixelShuffle(2)\n",
    "            )\n",
    "        else:\n",
    "            self.upsapling = nn.Sequential(\n",
    "                nn.Conv2d(n_feats, n_feats * self.scale * self.scale, 1, 1, 0),\n",
    "                nn.PixelShuffle(self.scale)\n",
    "            )\n",
    "\n",
    "        self.tail = nn.Conv2d(n_feats, 1, 3, 1, 1)\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.head(x)\n",
    "        x0 = self.fuse(torch.cat([x0, self.body(x0)], dim=1))\n",
    "        x0 = self.upsapling(x0)\n",
    "        x0 = self.tail(self.act(x0))\n",
    "        x = F.interpolate(x, scale_factor=self.scale, mode='bicubic', align_corners=False)\n",
    "        return (torch.tanh(x0 + x) +1.0)/2.0\n",
    "\n",
    "network = SRModel()\n",
    "inp = torch.rand([1,1,128,128])\n",
    "out = network(inp)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2895cd7d-f0f6-4b38-a329-f9ac097b2632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "network = network.to(device) #moving model on GPU or CPU, as per availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac08ca3-7510-40f9-aecb-78836d736291",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Blocks(nn.Module):\n",
    "    \"\"\"\n",
    "        Basic convolutional block used in the discriminator network.\n",
    "\n",
    "        Consists of:\n",
    "        - Convolution layer with stride\n",
    "        - Batch normalization\n",
    "        - LeakyReLU activation\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        in_channels : int\n",
    "            Number of input channels\n",
    "        out_channels : int\n",
    "            Number of output channels\n",
    "        stride : int\n",
    "            Convolution stride\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(Blocks, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "        Discriminator network for adversarial training in DEM super-resolution.\n",
    "\n",
    "        Architecture:\n",
    "        - Initial convolutional layer\n",
    "        - Series of convolutional blocks with increasing feature channels\n",
    "        - Final classification layer\n",
    "\n",
    "        The network progressively reduces spatial dimensions while increasing feature channels,\n",
    "        followed by a binary classification output.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        in_channels : int\n",
    "            Number of input channels\n",
    "        features : int\n",
    "            Base number of feature channels\n",
    "\n",
    "        Attributes:\n",
    "        -----------\n",
    "        first_layer : nn.Sequential\n",
    "            Initial feature extraction\n",
    "        Block1-Block9 : Blocks/nn.Sequential\n",
    "            Main processing blocks with different channel configurations\n",
    "        final_layer : nn.Sequential\n",
    "            Classification head with sigmoid activation\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, features):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.first_layer= nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features, 3, 2 ,1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.Block1 = Blocks(features, features*2, stride=2)\n",
    "        self.Block2 = Blocks(features*2, features*2, stride=1)\n",
    "        self.Block3 = Blocks(features*2, features*4, stride=2)\n",
    "        self.Block4 = Blocks(features*4, features*4, stride=1)\n",
    "        self.Block5 = Blocks(features*4, features*8, stride=2)\n",
    "        self.Block6 = Blocks(features*8, features*8, stride=1)\n",
    "        self.Block7 = Blocks(features*8, features*8, stride=2)\n",
    "        self.Block8 = Blocks(features*8, features*8, stride=2)\n",
    "        self.Block9 = nn.Sequential(\n",
    "            nn.Conv2d(features*8, features*4, 3, 2, 1),\n",
    "        \n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Linear(features*4, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x =  self.first_layer(x)\n",
    "        x =  self.Block1(x)\n",
    "        x =  self.Block2(x)\n",
    "        x =  self.Block3(x)\n",
    "        x =  self.Block4(x)\n",
    "        x =  self.Block5(x)\n",
    "        x =  self.Block6(x)\n",
    "        x =  self.Block7(x)\n",
    "        x =  self.Block8(x)\n",
    "        x = self.Block9(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.final_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ee1797-a685-4736-afd0-a0d22e42c471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Initializes and configures the super-resolution generator and discriminator models.\n",
    "\n",
    "    Setup includes:\n",
    "    1. Creating generator (SRModel) instance\n",
    "    2. Loading pre-trained generator weights\n",
    "    3. Creating discriminator instance\n",
    "    4. Moving both models to specified device (CPU/GPU)\n",
    "\n",
    "    Variables:\n",
    "    ----------\n",
    "    generator : SRModel\n",
    "        Super-resolution generator model\n",
    "    discriminator : Discriminator\n",
    "        Discriminator model for adversarial training\n",
    "    device : torch.device\n",
    "        Computation device (CPU/GPU)\n",
    "\"\"\"\n",
    "pre_trained_weight_path = 'model.pt'\n",
    "generator = SRModel()\n",
    "generator = generator.to(device)\n",
    "generator.load_state_dict(torch.load(pre_trained_weight_path))\n",
    "discriminator = Discriminator(1, 128) # 128 is the shape of input image, change it accordingly\n",
    "discriminator = discriminator.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dde91c3-6536-4d71-b3b6-2491f2a74853",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c83b29-5297-406f-9b35-3fdebf596425",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.init(project=\"data_Dem_pretrained_wt_mutlpleDownsampling\", name=\"EXP-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f035fb8-4816-4bfa-b399-6a0b83f0ca2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Sets up multiple loss functions and optimizers for training.\n",
    "\n",
    "    Loss Components:\n",
    "    - L1 Loss: For pixel-wise accuracy\n",
    "    - Edge Loss: For gradient preservation\n",
    "    - SSIM: For structural similarity\n",
    "    - Additional L1: For supplementary feature matching\n",
    "\n",
    "    Optimizers:\n",
    "    - Generator: Adam optimizer with learning rate 1e-5\n",
    "    - Discriminator: Adam optimizer with learning rate 1e-5\n",
    "\"\"\"\n",
    "\n",
    "l1Loss = nn.L1Loss().to(device) \n",
    "edgeLoss = gradientAwareLoss().to(device) \n",
    "ssim = SSIM(data_range=1.0).to(device) \n",
    "anotherl1Loss = nn.L1Loss().to(device) \n",
    "\n",
    "optim_G = torch.optim.Adam(generator.parameters(), lr=0.00001)\n",
    "optim_D = torch.optim.Adam(discriminator.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee194e-3880-4cf0-8b38-813143fe4095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sobelFilter = Sobel().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d051fab7-5b33-4ce4-9d43-c031c34e7873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchmetrics import StructuralSimilarityIndexMeasure as SSIM\n",
    "ssim = SSIM(data_range=1.0).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ec0d8d-29f8-4c54-9a43-ea63fa6e4c61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "num_train_batches = float(len(trainloader))\n",
    "num_val_batches = float(len(testloader))\n",
    "\n",
    "def train_one_epoch(epoch):\n",
    "    \n",
    "    \"\"\"\n",
    "        Performs one epoch of adversarial training for the super-resolution model.\n",
    "\n",
    "        The training process includes:\n",
    "        1. Generator training with combined losses:\n",
    "           - Adversarial loss\n",
    "           - L1 reconstruction loss\n",
    "           - SSIM loss\n",
    "           - Edge preservation loss\n",
    "        2. Discriminator training with real/fake image classification\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        epoch : int\n",
    "            Current epoch number\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Average PSNR value for the epoch\n",
    "\n",
    "        Logging:\n",
    "        --------\n",
    "        - L1 Loss\n",
    "        - Edge Loss\n",
    "        - SSIM Loss\n",
    "        - Total Loss\n",
    "        - SSIM\n",
    "        - PSNR\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Epoch {epoch}: \", end =\"\")\n",
    "    \n",
    "    l1_loss_per_epoch = 0.0\n",
    "    edge_loss_per_epoch = 0.0\n",
    "    ssim_loss_per_epoch = 0.0\n",
    "    ssim_per_epoch = 0.0\n",
    "    psnr_per_epoch = 0.0\n",
    "    total_loss_per_epoch = 0.0\n",
    "    D_adv_loss = 0\n",
    "    \n",
    "    generator.train()\n",
    "    for batch, (hr, lr) in enumerate(tqdm(trainloader)):\n",
    "\n",
    "        for p in discriminator.parameters():\n",
    "            p.requires_grad = False\n",
    "        #training generator\n",
    "        optim_G.zero_grad()\n",
    " \n",
    "        lr_images = lr.to(device)\n",
    "        hr_images = hr.to(device)\n",
    "        lr_images = lr_images.float()\n",
    "        predicted_hr_images = generator(lr_images)\n",
    "        predicted_hr_labels = discriminator(predicted_hr_images)\n",
    "        gf_loss = F.binary_cross_entropy_with_logits(predicted_hr_labels, torch.ones_like(predicted_hr_labels)) #adverserial loss\n",
    "      \n",
    "      \n",
    "        # reconstruction loss    \n",
    "      \n",
    "        l1_loss_per_sample = l1Loss(hr_images*1000, predicted_hr_images*1000)\n",
    "        ssim_per_sample = ssim(hr_images, predicted_hr_images)\n",
    "        ssim_loss_per_sample = 1 - ssim_per_sample\n",
    "        edge_loss = edgeLoss(hr_images*1000, predicted_hr_images*1000)  \n",
    "        reconstruction_loss = l1_loss_per_sample + 100*(ssim_loss_per_sample) + 50*edge_loss\n",
    "        t_loss = reconstruction_loss + 50*gf_loss\n",
    "        \n",
    "      \n",
    "        t_loss.backward()\n",
    "        optim_G.step()\n",
    "      \n",
    "        psnr_per_sample = calculate_psnr(hr_images.detach().cpu().numpy(), predicted_hr_images.detach().cpu().numpy())\n",
    "    \n",
    "        l1_loss_per_epoch += l1_loss_per_sample.item()\n",
    "        edge_loss_per_epoch += edge_loss.item() \n",
    "        ssim_loss_per_epoch += ssim_loss_per_sample.item() \n",
    "        ssim_per_epoch += ssim_per_sample.item()\n",
    "        psnr_per_epoch += psnr_per_sample \n",
    "        total_loss_per_epoch += t_loss.item()\n",
    "      \n",
    "        # training discriminator\n",
    "        for p in discriminator.parameters():\n",
    "            p.requires_grad = True\n",
    "        optim_D.zero_grad()\n",
    "        predicted_hr_images = generator(lr_images).detach() # avoid back propogation to generator\n",
    "        hr_images = hr_images.float()\n",
    "        adv_hr_real = discriminator(hr_images)\n",
    "        adv_hr_fake = discriminator(predicted_hr_images)\n",
    "        df_loss = F.binary_cross_entropy_with_logits(adv_hr_real, torch.ones_like(adv_hr_real)) + F.binary_cross_entropy_with_logits(adv_hr_fake, torch.zeros_like(adv_hr_fake))\n",
    "        D_adv_loss += df_loss.item()\n",
    "        df_loss.backward()\n",
    "        optim_D.step()\n",
    "    \n",
    "    l1_loss_per_epoch /= float(len(trainloader))\n",
    "    edge_loss_per_epoch /= float(len(trainloader))\n",
    "    ssim_loss_per_epoch /= float(len(trainloader))\n",
    "    ssim_per_epoch /= float(len(trainloader))\n",
    "    psnr_per_epoch /= float(len(trainloader))\n",
    "    total_loss_per_epoch /= float(len(trainloader))\n",
    "    \n",
    "    wandb.log({\"Train L1 Loss\": l1_loss_per_epoch})\n",
    "    wandb.log({\"Train Edge Loss\": edge_loss_per_epoch})\n",
    "    wandb.log({\"Train SSIM Loss\": ssim_loss_per_epoch})\n",
    "    wandb.log({\"Train Total Loss\": total_loss_per_epoch})\n",
    "    wandb.log({\"Train SSIM\": ssim_per_epoch})\n",
    "    wandb.log({\"Train PSNR\": psnr_per_epoch})\n",
    "    \n",
    "    print(f\"(Train) L1 Loss: {l1_loss_per_epoch:.3f} | SSIM Loss: {ssim_loss_per_epoch:.3f} | Edge Loss: {edge_loss_per_epoch:.3f} | Total Loss: {total_loss_per_epoch:.3f}\")\n",
    "    print(f\"SSIM: {ssim_per_epoch:.3f} | PSNR: {psnr_per_epoch}\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "    return psnr_per_epoch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a999e19e-041e-4ceb-a1db-46e3cdcd2932",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def valid_one_epoch(epoch):\n",
    "    \"\"\"\n",
    "        Performs one epoch of validation for the super-resolution model.\n",
    "\n",
    "        The validation process includes:\n",
    "        1. Forward pass through generator\n",
    "        2. Calculation of SSIM and PSNR metrics\n",
    "        3. Visualization of sample results (LR, HR, and SR images)\n",
    "        4. Logging of metrics and visualizations\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        epoch : int\n",
    "            Current epoch number\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Average PSNR value for the validation set\n",
    "\n",
    "        Logging:\n",
    "        --------\n",
    "        - SSIM\n",
    "        - PSNR\n",
    "        - Sample image visualizations (LR, HR, SR)\n",
    "    \"\"\"\n",
    "    ssim_per_epoch = 0.0\n",
    "    psnr_per_epoch = 0.0\n",
    "    b_ssim_per_epoch = 0.0\n",
    "    b_psnr_per_epoch = 0.0\n",
    "    \n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        for hr, lr in tqdm(testloader):\n",
    "            batched_hr, batched_lr = hr.to(device), lr.to(device)\n",
    "            predicted_sr = generator(batched_lr)\n",
    "\n",
    "            ssim_per_epoch += ssim(batched_hr, predicted_sr)\n",
    "            psnr_per_epoch += calculate_psnr(batched_hr.cpu().numpy(), predicted_sr.cpu().numpy())\n",
    "\n",
    "            grid1 = make_grid(batched_lr[:4])\n",
    "            grid2 = make_grid(batched_hr[:4])\n",
    "            grid3 = make_grid(predicted_sr[:4])\n",
    "\n",
    "\n",
    "            grid1 = wandb.Image(grid1, caption=\"Low Resolution DEM\")\n",
    "            grid2 = wandb.Image(grid2, caption=\"High Resolution DEM\")\n",
    "            grid3 = wandb.Image(grid3, caption=\"Reconstructed High Resolution DEM\")\n",
    "\n",
    "            wandb.log({\"Original LR\": grid1})\n",
    "            wandb.log({\"Original HR\": grid2})\n",
    "            wandb.log({\"Reconstruced\": grid3})\n",
    "\n",
    "\n",
    "        ssim_per_epoch /= float(len(testloader))\n",
    "        psnr_per_epoch /= float(len(testloader))\n",
    "\n",
    "\n",
    "        wandb.log({\"Test Predicted SSIM\": ssim_per_epoch})\n",
    "        wandb.log({\"Test Predicted PSNR\": psnr_per_epoch})\n",
    "       \n",
    "\n",
    "        print(f\"(Val) SSIM: {ssim_per_epoch:.3f} | PSNR: {psnr_per_epoch:.3f}\")\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return psnr_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19397489-b79a-4063-89ed-af85003c8013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Training Loop Configuration\n",
    "\"\"\"\n",
    "    Main training loop for the super-resolution model with model saving logic.\n",
    "\n",
    "    Components:\n",
    "    - Training for specified number of epochs\n",
    "    - Validation after each epoch\n",
    "    - Model checkpoint saving based on best PSNR\n",
    "    - Memory management with CUDA cache clearing\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "best_psnr = 0\n",
    "prev_psnr =0\n",
    "expNumber = 5\n",
    "num_epochs = 1\n",
    "os.makedirs(f\"savedModels/Exp{expNumber}\",exist_ok=True)\n",
    "for i in range(num_epochs):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    train_psnr = train_one_epoch(i)\n",
    "    valid_psnr = valid_one_epoch(i)\n",
    "\n",
    "    \n",
    "    if valid_psnr > best_psnr:\n",
    "        best_psnr = valid_psnr\n",
    "        torch.save(generator.state_dict(), f\"savedModels/Exp{expNumber}/SRmodel_{best_psnr}.pt\")\n",
    "        print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582e654d-1392-4884-bfbe-e8dfc9044f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
