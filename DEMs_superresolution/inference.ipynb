{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d223e3-2e0b-44e6-824a-2f5d3a76e19b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654ee535-0899-4c22-a85e-568fef0b3004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DEMDataset(Dataset):\n",
    "    \"\"\"Dataset class for handling DEM files\"\"\"\n",
    "    def __init__(self, \n",
    "                 data_dir,\n",
    "                 batch_size=4):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Get all .pt files\n",
    "        self.file_list = glob.glob(self.data_dir+\"*.pt\")\n",
    "        if '../data_DEMs_no_nan/test/NAC_DTM_MESSIER3_block_211.pt' in self.file_list:\n",
    "            self.file_list.remove('../data_DEMs_no_nan/test/NAC_DTM_MESSIER3_block_211.pt')\n",
    "        \n",
    "        # Store normalization parameters for each file\n",
    "        self.norm_params = {}\n",
    "        self._compute_normalization_params()\n",
    "        \n",
    "    def _compute_normalization_params(self):\n",
    "        \"\"\"Compute min and max values for each DEM file\"\"\"\n",
    "        for file_name in self.file_list:\n",
    "            data = torch.load(file_name)\n",
    "            self.norm_params[file_name] = {\n",
    "                'min': float(data.min()),\n",
    "                'max': float(data.max())\n",
    "            }\n",
    "                \n",
    "    def normalize(self, data, file_name):\n",
    "        \"\"\"Normalize data to range [0, 1]\"\"\"\n",
    "        min_val = self.norm_params[file_name]['min']\n",
    "        max_val = self.norm_params[file_name]['max']\n",
    "        return (data - min_val) / (max_val - min_val)\n",
    "    \n",
    "    def unnormalize(self, data, file_name):\n",
    "        \"\"\"Un-normalize data back to original scale\"\"\"\n",
    "        min_val = self.norm_params[file_name]['min']\n",
    "        max_val = self.norm_params[file_name]['max']\n",
    "        return data * (max_val - min_val) + min_val\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_list[idx]\n",
    "        # file_path = os.path.join(self.data_dir, file_name)\n",
    "        \n",
    "        im = torch.load(file_path)\n",
    "        \n",
    "        if not isinstance(im, torch.Tensor):\n",
    "            im = torch.tensor(im, dtype=torch.float32)\n",
    "        \n",
    "        im = self.normalize(im, file_path)\n",
    "        \n",
    "        if im.dim() == 2:\n",
    "            im = im.unsqueeze(0)\n",
    "        \n",
    "        HR = im.float()\n",
    "        LR = self._bilinear_downsample(HR)\n",
    "        \n",
    "        HR = HR.squeeze()\n",
    "        LR = LR.squeeze().float()\n",
    "        \n",
    "        data = (HR,LR)\n",
    "\n",
    "        return data, file_path\n",
    "    \n",
    "    def _bilinear_downsample(self, HR):\n",
    "        \"\"\"Bilinear downsampling\"\"\"\n",
    "        return torch.nn.functional.interpolate(HR.unsqueeze(0), scale_factor=0.5, mode='bilinear').squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267dfcbd-a2a5-4e31-96c9-f71c3e10eafb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dem_dataloader(data_dir,\n",
    "                      batch_size = 32,\n",
    "                      shuffle = False):\n",
    "    \"\"\"\n",
    "    Create DataLoader for DEM files\n",
    "    \n",
    "    Returns:\n",
    "        DataLoader and Dataset objects\n",
    "    \"\"\"\n",
    "    dataset = DEMDataset(data_dir, batch_size)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return dataloader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa73eec-071f-4df4-85ba-6681fa8da380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data_dir = 'presentation_output/mm_tile_input/'\n",
    "testloader,testset = get_dem_dataloader(data_dir=test_data_dir,batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c6bd8e-966e-42ee-985b-ff318165e611",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q einops\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "from math import exp\n",
    "from tqdm import tqdm\n",
    "\n",
    "from kornia.filters.sobel import Sobel\n",
    "import wandb\n",
    "from torchvision.utils import make_grid\n",
    "import gc\n",
    "from torchmetrics import StructuralSimilarityIndexMeasure as SSIM\n",
    "\n",
    "\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from einops import rearrange\n",
    "from torch.nn import init \n",
    "\n",
    "\n",
    "class depthwise_separable_conv(nn.Module):\n",
    "    def __init__(self, nin, nout, kernel_size = 3, padding = 1, bias=False):\n",
    "        super(depthwise_separable_conv, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(nin, nin, kernel_size=kernel_size, padding=padding, groups=nin, bias=bias)\n",
    "        self.pointwise = nn.Conv2d(nin, nout, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "\n",
    "class ERAM(nn.Module):\n",
    "    def __init__(self, channel_begin, dimension):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channel_begin, channel_begin, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.avgpool = nn.AvgPool2d(dimension)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(channel_begin, channel_begin//2, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(channel_begin//2, channel_begin, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(channel_begin, channel_begin, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.dconv = depthwise_separable_conv(channel_begin, channel_begin, kernel_size = 3, padding = 1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        si_ca = self.avgpool(x) + torch.var_mean(x, dim=(2,3))[0].unsqueeze(2).unsqueeze(2)\n",
    "        mi_ca = self.conv2(self.relu(self.conv1(si_ca)))\n",
    "\n",
    "        mi_sa = self.conv3(self.relu(self.dconv(x)))\n",
    "\n",
    "        return self.sigmoid(mi_ca+mi_sa) * x\n",
    "\n",
    "    \n",
    "\n",
    "class SelfAttn(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, bias=False):\n",
    "        super(SelfAttn, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=bias)\n",
    "        self.proj_out = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, N, c = x.shape\n",
    "\n",
    "        qkv = self.qkv(x).chunk(3, dim=-1)\n",
    "        # [b, N, c] -> [b, N, head, c//head] -> [b, head, N, c//head]\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.num_heads), qkv)\n",
    "\n",
    "        # [b, head, N, c//head] * [b, head, N, c//head] -> [b, head, N, N]\n",
    "        attn = torch.einsum('bijc, bikc -> bijk', q, k) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        # [b, head, N, N] * [b, head, N, c//head] -> [b, head, N, c//head] -> [b, N, head, c//head]\n",
    "        x = torch.einsum('bijk, bikc -> bijc', attn, v)\n",
    "        x = rearrange(x, 'b i j c -> b j (i c)')\n",
    "        x = self.proj_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, mlp_ratio=4):\n",
    "        super(Mlp, self).__init__()\n",
    "        hidden_features = in_features * mlp_ratio\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_features),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_features, in_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (b, h, w, c)\n",
    "        window_size (int): window size\n",
    "    Returns:\n",
    "        windows: (num_windows*b, window_size, window_size, c) [non-overlap]\n",
    "    \"\"\"\n",
    "    return rearrange(x, 'b (h s1) (w s2) c -> (b h w) s1 s2 c', s1=window_size, s2=window_size)\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, h, w):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        h (int): Height of image\n",
    "        w (int): Width of image\n",
    "    Returns:\n",
    "        x: (b, h, w, c)\n",
    "    \"\"\"\n",
    "    b = int(windows.shape[0] / (h * w / window_size / window_size))\n",
    "    return rearrange(windows, '(b h w) s1 s2 c -> b (h s1) (w s2) c', b=b, h=h // window_size, w=w // window_size)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, num_heads=4, window_size=8, mlp_ratio=4, qkv_bias=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.pos_embed = nn.Conv2d(dim, dim, 3, padding=1, groups=dim)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = SelfAttn(dim, num_heads, qkv_bias)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "        self.mlp = Mlp(dim, mlp_ratio)\n",
    "        # self.rrdBAtt = RRDBAttention(256,32)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_embed(x)\n",
    "        x = rearrange(x, 'b c h w -> b h w c')\n",
    "        b, h, w, c = x.shape\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        pad_l = pad_t = 0\n",
    "        pad_r = (self.window_size - w % self.window_size) % self.window_size\n",
    "        pad_b = (self.window_size - h % self.window_size) % self.window_size\n",
    "        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "        _, Hp, Wp, _ = x.shape\n",
    "\n",
    "        x_windows = window_partition(x, self.window_size)  # nW*B, window_size, window_size, c\n",
    "        x_windows = rearrange(x_windows, 'B s1 s2 c -> B (s1 s2) c', s1=self.window_size,\n",
    "                              s2=self.window_size)  # nW*b, window_size*window_size, c\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        # print(\"shape before : \",x_windows.shape)\n",
    "        attn_windows = self.attn(x_windows)  # nW*b, window_size*window_size, c\n",
    "\n",
    "        # x_windows = x_windows.view(b,256,64,-1)\n",
    "\n",
    "        # attn_windows = self.rrdBAtt(x_windows)\n",
    "\n",
    "        attn_windows = attn_windows.reshape(b*256,64,-1)\n",
    "\n",
    "        # print(\"shape after : \",x_windows.shape)\n",
    "        # merge windows\n",
    "        attn_windows = rearrange(attn_windows, 'B (s1 s2) c -> B s1 s2 c', s1=self.window_size, s2=self.window_size)\n",
    "        x = window_reverse(attn_windows, self.window_size, Hp, Wp)  # b H' W' c\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if pad_r > 0 or pad_b > 0:\n",
    "            x = x[:, :h, :w, :].contiguous()\n",
    "\n",
    "        x = x + shortcut\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return rearrange(x, 'b h w c -> b c h w')\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_features, ratio=4):\n",
    "        super(ResBlock, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_features, in_features * ratio, 1, 1, 0),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_features * ratio, in_features * ratio, 3, 1, 1, groups=in_features * ratio),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_features * ratio, in_features, 1, 1, 0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        return self.net(x) + x\n",
    "\n",
    "\n",
    "class BaseBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, window_size=8, ratios=[1, 2, 2, 4, 4], qkv_bias=False):\n",
    "        super(BaseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        # self.eram = ERAM(dim,128)\n",
    "#         print('dim : ',dim)\n",
    "        for ratio in ratios:\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Transformer(dim, num_heads, window_size, ratio, qkv_bias),\n",
    "                ResBlock(dim, ratio),\n",
    "                ERAM(dim,128)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for tblock, rblock ,eram in self.layers:\n",
    "            x = tblock(x)\n",
    "            x = rblock(x)\n",
    "            x = eram(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SRModel(nn.Module):\n",
    "    def __init__(self, n_feats=40, n_heads=8, ratios=[4, 2, 2, 2, 4], upscaling_factor=2):\n",
    "        super(SRModel, self).__init__()\n",
    "        self.scale = upscaling_factor\n",
    "        self.head = nn.Conv2d(1, n_feats, 3, 1, 1)\n",
    "\n",
    "        self.body = BaseBlock(n_feats, num_heads=n_heads, ratios=ratios)\n",
    "\n",
    "        self.fuse = nn.Conv2d(n_feats * 2, n_feats, 3, 1, 1)\n",
    "\n",
    "        if self.scale == 4:\n",
    "            self.upsapling = nn.Sequential(\n",
    "                nn.Conv2d(n_feats, n_feats * 4, 1, 1, 0),\n",
    "                nn.PixelShuffle(2),\n",
    "                nn.Conv2d(n_feats, n_feats * 4, 1, 1, 0),\n",
    "                nn.PixelShuffle(2)\n",
    "            )\n",
    "        else:\n",
    "            self.upsapling = nn.Sequential(\n",
    "                nn.Conv2d(n_feats, n_feats * self.scale * self.scale, 1, 1, 0),\n",
    "                nn.PixelShuffle(self.scale)\n",
    "            )\n",
    "\n",
    "        self.tail = nn.Conv2d(n_feats, 1, 3, 1, 1)\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.head(x)\n",
    "        x0 = self.fuse(torch.cat([x0, self.body(x0)], dim=1))\n",
    "        x0 = self.upsapling(x0)\n",
    "        x0 = self.tail(self.act(x0))\n",
    "        x = F.interpolate(x, scale_factor=self.scale, mode='bicubic', align_corners=False)\n",
    "        return (torch.tanh(x0 + x) +1.0)/2.0\n",
    "\n",
    "network = SRModel()\n",
    "\n",
    "inp = torch.rand([1,1,128,128])\n",
    "out = network(inp)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93012eda-523a-4a28-bb77-992d36338046",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_dem_data(data_dir,\n",
    "                    model,\n",
    "                    device,\n",
    "                    batch_size):\n",
    "    \"\"\"\n",
    "    Process DEM data through the super-resolution model\n",
    "    \"\"\"\n",
    "\n",
    "    dataloader, dataset = get_dem_dataloader(data_dir, batch_size)\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = {}\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch, file_names in dataloader:\n",
    "            # Move hr,lr to device\n",
    "            HR,LR = batch\n",
    "            HR = HR.unsqueeze(1).to(device)\n",
    "            LR = LR.unsqueeze(1).to(device)\n",
    "            \n",
    "            output = model(LR)\n",
    "            \n",
    "            # Move output back to CPU and unnormalize\n",
    "            output = output.cpu()\n",
    "            \n",
    "            # Store unnormalized results\n",
    "            for i, file_name in enumerate(file_names):\n",
    "                lr_unnorm = dataset.unnormalize(LR[i], file_name)\n",
    "                hr_unnorm = dataset.unnormalize(HR[i], file_name)\n",
    "                output_unnorm = dataset.unnormalize(output[i], file_name)\n",
    "                results[file_name] = {\n",
    "                    'lr': lr_unnorm,\n",
    "                    'hr': hr_unnorm,\n",
    "                    'output': output_unnorm\n",
    "                }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b680be9c-59ef-4b10-93c9-492553a60507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = 'fintunedWeight_MultiDownsampling.pt'\n",
    "test_data_dir = 'presentation_output/mm_tile_input/'\n",
    "network.to('cuda')\n",
    "network.load_state_dict(torch.load(model_path))\n",
    "dem_results = process_dem_data(data_dir=test_data_dir,model = network, device='cuda',batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042376a0-1ed4-4de4-b50d-c6a0a2a95d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_results(results, filename, figsize=(20, 5)):\n",
    "    \"\"\"\n",
    "    Visualize LR, HR, SR images and the difference map (HR-SR) side by side\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Dictionary containing LR, HR, and SR images\n",
    "        filename (str): Filename for the title\n",
    "        figsize (tuple): Figure size\n",
    "    \"\"\"\n",
    "\n",
    "    lr_img = results[filename]['lr'].squeeze().cpu().numpy()\n",
    "    hr_img = results[filename]['hr'].squeeze().cpu().numpy()\n",
    "    sr_img = results[filename]['output'].squeeze().cpu().numpy()\n",
    "    \n",
    "    diff_map = np.abs(hr_img - sr_img)\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    gs = plt.GridSpec(1, 4, width_ratios=[1, 2, 2, 2])\n",
    "    \n",
    "    ax0 = plt.subplot(gs[0])\n",
    "    im0 = ax0.imshow(lr_img, cmap='terrain')\n",
    "    ax0.set_title(f'LR ({lr_img.shape[0]}x{lr_img.shape[1]})')\n",
    "    plt.colorbar(im0, ax=ax0)\n",
    "    \n",
    "    ax1 = plt.subplot(gs[1])\n",
    "    im1 = ax1.imshow(hr_img, cmap='terrain')\n",
    "    ax1.set_title(f'HR ({hr_img.shape[0]}x{hr_img.shape[1]})')\n",
    "    plt.colorbar(im1, ax=ax1)\n",
    "    \n",
    "    ax2 = plt.subplot(gs[2])\n",
    "    im2 = ax2.imshow(sr_img, cmap='terrain')\n",
    "    ax2.set_title(f'SR ({sr_img.shape[0]}x{sr_img.shape[1]})')\n",
    "    plt.colorbar(im2, ax=ax2)\n",
    "    \n",
    "    ax3 = plt.subplot(gs[3])\n",
    "    im3 = ax3.imshow(diff_map, cmap='RdYlBu')\n",
    "    ax3.set_title(f'|HR - SR|\\nAvg diff: {diff_map.mean():.4f}')\n",
    "    plt.colorbar(im3, ax=ax3)\n",
    "    \n",
    "    plt.suptitle(f'Comparison for {os.path.basename(filename)}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def visualize_multiple_samples(results, num_samples=3, figsize=(15, 5)):\n",
    "    \"\"\"\n",
    "    Visualize multiple samples from the results\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Dictionary containing results for all samples\n",
    "        num_samples (int): Number of samples to visualize\n",
    "        figsize (tuple): Base figure size for each sample\n",
    "    \"\"\"\n",
    "    filenames = list(results.keys())\n",
    "    \n",
    "    # Select random samples if there are more than num_samples\n",
    "    if len(filenames) > num_samples:\n",
    "        filenames = np.random.choice(filenames, num_samples, replace=False)\n",
    "    \n",
    "    figs = []\n",
    "    for filename in filenames:\n",
    "        fig = visualize_results(results, filename, figsize=figsize)\n",
    "        figs.append(fig)\n",
    "        plt.show()\n",
    "    \n",
    "    return figs\n",
    "\n",
    "def visualize(data_dir, model, device, batch_size, num_samples=3):\n",
    "    \"\"\"\n",
    "    visualize results\n",
    "    \"\"\"\n",
    "    results = process_dem_data(data_dir, model, device, batch_size)\n",
    "    figs = visualize_multiple_samples(results, num_samples=num_samples)\n",
    "    \n",
    "    return figs\n",
    "\n",
    "figs = visualize(\n",
    "    data_dir='presentation_output/mm_tile_input/',\n",
    "    model=network,\n",
    "    device='cuda',\n",
    "    batch_size=4,\n",
    "    num_samples=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38303e47-0a1a-4a47-8e48-153f88d71481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_and_save_results(results, filename, save_dir, figsize=(20, 5)):\n",
    "    \"\"\"\n",
    "    Visualize and save LR, HR, SR images and the difference map separately\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Dictionary containing the images\n",
    "        filename (str): Original filename\n",
    "        save_dir (str): Directory to save the images\n",
    "        figsize (tuple): Figure size for combined plot\n",
    "    \"\"\"\n",
    "    # Create save directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Get base filename without extension and directory\n",
    "    base_filename = os.path.splitext(os.path.basename(filename))[0]\n",
    "    \n",
    "    # Get images\n",
    "    lr_img = results[filename]['lr'].squeeze().cpu().numpy()\n",
    "    hr_img = results[filename]['hr'].squeeze().cpu().numpy()\n",
    "    sr_img = results[filename]['output'].squeeze().cpu().numpy()\n",
    "    \n",
    "    # Calculate difference map\n",
    "    diff_map = np.abs(hr_img - sr_img)\n",
    "    avg_diff = np.mean(diff_map)\n",
    "    \n",
    "    # Function to save individual plots\n",
    "    def save_subplot(img, title, filename, cmap='terrain'):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        im = plt.imshow(img, cmap=cmap)\n",
    "        plt.colorbar(im)\n",
    "        plt.title(title)\n",
    "        plt.savefig(filename, bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # Save individual plots\n",
    "    save_subplot(\n",
    "        lr_img, \n",
    "        f'LR ({lr_img.shape[0]}x{lr_img.shape[1]})', \n",
    "        os.path.join(save_dir, f'{base_filename}_LR.png')\n",
    "    )\n",
    "    \n",
    "    save_subplot(\n",
    "        hr_img, \n",
    "        f'HR ({hr_img.shape[0]}x{hr_img.shape[1]})', \n",
    "        os.path.join(save_dir, f'{base_filename}_HR.png')\n",
    "    )\n",
    "    \n",
    "    save_subplot(\n",
    "        sr_img, \n",
    "        f'SR ({sr_img.shape[0]}x{sr_img.shape[1]})', \n",
    "        os.path.join(save_dir, f'{base_filename}_SR.png')\n",
    "    )\n",
    "    \n",
    "    save_subplot(\n",
    "        diff_map, \n",
    "        f'|HR - SR|\\nAvg diff: {avg_diff:.4f}', \n",
    "        os.path.join(save_dir, f'{base_filename}_DIFF.png'),\n",
    "        cmap='RdYlBu'\n",
    "    )\n",
    "    \n",
    "    # Create and save combined plot\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gs = plt.GridSpec(1, 4, width_ratios=[1, 2, 2, 2])\n",
    "    \n",
    "    # Plot LR image\n",
    "    ax0 = plt.subplot(gs[0])\n",
    "    im0 = ax0.imshow(lr_img, cmap='terrain')\n",
    "    ax0.set_title(f'Input: Low Resolution ({lr_img.shape[0]}x{lr_img.shape[1]})')\n",
    "    plt.colorbar(im0, ax=ax0)\n",
    "    \n",
    "    # Plot HR image\n",
    "    ax1 = plt.subplot(gs[1])\n",
    "    im1 = ax1.imshow(hr_img, cmap='terrain')\n",
    "    ax1.set_title(f'HR ({hr_img.shape[0]}x{hr_img.shape[1]})')\n",
    "    plt.colorbar(im1, ax=ax1)\n",
    "    \n",
    "    # Plot SR image\n",
    "    ax2 = plt.subplot(gs[2])\n",
    "    im2 = ax2.imshow(sr_img, cmap='terrain')\n",
    "    ax2.set_title(f'Output: Super Resolution ({sr_img.shape[0]}x{sr_img.shape[1]})')\n",
    "    plt.colorbar(im2, ax=ax2)\n",
    "    \n",
    "    # Plot difference map\n",
    "    ax3 = plt.subplot(gs[3])\n",
    "    im3 = ax3.imshow(diff_map, cmap='RdYlBu')\n",
    "    ax3.set_title(f'|GT - Output|\\nAvg diff: {avg_diff:.4f}')\n",
    "    plt.colorbar(im3, ax=ax3)\n",
    "    \n",
    "    plt.suptitle(f'Comparison for {base_filename}')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save combined plot\n",
    "    fig.savefig(os.path.join(save_dir, f'{base_filename}_combined.png'), \n",
    "                bbox_inches='tight', dpi=600)\n",
    "    plt.close(fig)\n",
    "\n",
    "def save_visualizations(results, save_dir, num_samples=None):\n",
    "    \"\"\"\n",
    "    Process and save visualizations for multiple samples\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Dictionary containing all results\n",
    "        save_dir (str): Directory to save the images\n",
    "        num_samples (int): Number of samples to process (None for all)\n",
    "    \"\"\"\n",
    "    filenames = list(results.keys())\n",
    "    \n",
    "    if num_samples is not None and num_samples < len(filenames):\n",
    "        filenames = np.random.choice(filenames, num_samples, replace=False)\n",
    "    \n",
    "    for filename in filenames:\n",
    "        print(f\"Processing {os.path.basename(filename)}...\")\n",
    "        visualize_and_save_results(results, filename, save_dir)\n",
    "\n",
    "\n",
    "save_dir = 'presentation_output/mm_output'\n",
    "save_visualizations(\n",
    "    results=dem_results,  \n",
    "    save_dir=save_dir,\n",
    "    num_samples=len(glob.glob(test_data_dir+\"*.pt\"))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nasa_super_res_pytorch)",
   "language": "python",
   "name": "nasa_tt_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
